{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./images/logo_nao_digital.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Interactuando con LLM's para el análisis de información \n",
    "\n",
    "## 1. Objetivo\n",
    "\n",
    "El presente reporte tiene por objeto introducir los elementos básicos que nos permitan entender las capacidades de los Large Language Models o LLM's (como ChatGPT, Gemini, Claude y otros), los principios en los que se basa dichas herramientas y la forma en que podemos interactuar con el mismo para analizar información texto a partir de su API (interfaz de programación de aplicaciones, por sus siglas en inglés).\n",
    "\n",
    "En adición, también se presentarán algunas librerias para interactuar con LLM's a partir de información en texto presente en documentos con diferentes formatos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Elementos sobre Large Language Models (LLM's)\n",
    "\n",
    "## 2.1 ¿Qué son los LLM's?\n",
    "\n",
    "Los Large Language Models (LLM's) son una serie de modelos que pertenecen a la rama de la inteligencia artificial de tipo generativa. Tales abordan la generación de contenido a partir de texto de entrada o de un conjunto de instrucciones dadas por un usuario, como un problema de predicción de sequencias de texto; es decir, estas herramientas intenta predecir cual es el texto que sea más acorde a una colección de texto ordenado. Estos modelos se basan en técnicas de aprendizaje profundo que se entrenan en grandes cantidades de texto de fuentes públicas como libros, comentarios de páginas web, redes sociales y foros, que por diseño abstraen el contenido vertido en tales datos y lo usan para predecir secuencias de texto, es decir, como una especie de base de conocimiento con el cual predecir cual sería la respuesta más acertada a cierta instrucción de texto.\n",
    "\n",
    "Para ilustralo, pensemos que queremos entrenar un modelo que sea capaz de responder cual es la siguiente palabra de la oración \"Añade a la olla la\". Proveyendo suficientes ejemplos, quizá incluyendo textos que tengan que ver con temas de temas de la vida cotidiana, podriamos pensar en respuestas como a) llanta, b) dinosaurio, c) Saturno, d) cebolla. Intutivamente sabemos que con los incisos a) y d) podríamos tener una oración coherente, pero el primer inciso redundaria en una oración extraña (*Añade a la olla la llanta*), mientras que el resto de incisos no parece tener relación alguna con un texto con significado aceptable en el idioma. En este caso un modelo de tipo LLM, calcularía un valor de probabilidad de cual es la palabra que sigue a la oración de entrada para generar el texto correspondiente. \n",
    "\n",
    "Ésta idea sencilla que es la base con la que funciona dicha arquitectura de modelos, es central para consolidar buenas prácticas de *prompt engineering*, dado que intutivamente nos dice que si queremos obtener resultados adecuados al darle instrucciones a un modelo generativo, necesitamos ser cuidados en los patrones de texto que introducimos, así el modelo generará texto resultado a partir de predicciones basadas en el contexto y la estructura que le hemos provisto de inicio.\n",
    "\n",
    "Por otro lado, vale la pena destacar que estos modelos se asocian a la palabra inglesa *Large* por dos motivos:\n",
    "\n",
    "1) Generalmente necesitan un cantidad de datos ejemplo de volumen enorme, como datos de foros de internet o post de redes sociales y\n",
    "2) Cuentan con una cantidad enorme de parámetros a ser calibrados (los modelos actuales rondan el orden de billones de parámetros). Por su puesto, un entrenamiento de modelos requiere de amplio poder computacional.\n",
    "\n",
    "Cabe destacar que en la actualidad, los modelos generativos y particularmente los LLM son un campo activo de investigación. De hecho, trabajos recientes de investigación también se han enfocado en desarrollar modelos LLM a través de los cuales se puedan sostener conversaciones coherentes e informativas entre personas y máquinas, además muchas organizaciones han desarrollado y adaptado diferentes LLM's para su uso, abarcando ámbitos de investigación, creatividad, moda, análisis de datos y muchos otros."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este sentido, es dable mencionar que entre las organizaciones han implementado sus propios LLM's y que los han hecho disponibles al público destacan las siguientes:\n",
    "\n",
    "i) [ChatGPT](https://chat.openai.com/) en una herramienta de inteligencia articifial lanzada en el año 2022 por la empresa OpenAI. Su funcionamiento es el de un chatbot interactivo capaz de mantener conversaciones con usuarios, responder preguntas y proporcionar información relevante sobre una variedad de temas.\n",
    "\n",
    "ii) [Google Gemini](https://gemini.google.com/) es un modelo desarrollado por la división DeepMind de Google, que es de tipo Multimodal (es decir, puede recibir inputs de muchas formas, como texto, imágenes y otros) y que salió a finales de 2023.\n",
    "\n",
    "ii) [Claude de Anthropic](https://www.anthropic.com/claude) es una familia de modelos entrenados por la compañia Anthropic para ser un asistente de texto.\n",
    "\n",
    "\n",
    "La siguiente imagen es una recopilación que ilustra, para marzo de 2024 a los principales actores del mercado de LLM's, entre los que se incluyen aquellos que dan acceso a sus capas de infraestructura para intectuar con los modelos o bien que han disponibilizado una aplicación de servicios basado en ellos para interacción del usuario:\n",
    "\n",
    "![title](./images/llm_market.jpeg)\n",
    "\n",
    "\n",
    "## 2.2 ¿Cuál es la teoría detrás de los LLM's?\n",
    "\n",
    "Los LLM's se basan en modelos de aprendizaje profundo, llamados *redes neuronales*, que se especializan en aprender la información subyacentes de un conjunto de datos de texto que se les proveen como ejemplo y luego usar ese conocimiento para generar nuevos datos relacionados con las preguntas que reciben como entrada.\n",
    "\n",
    "El diseño de tales redes utiliza una arquitectura de red neural llamada GPT (*Generative Pre-trained Transformer*), que permite capturar patrones complejos del lenguaje humano (como similaridad en significado, importancia de la posición del texto, etcétera).\n",
    "\n",
    "El punto de partida de tal proceso es entrenar un modelo al que se le presentan grandes cantidades de texto de ejemplo para aprender patrones y estructuras del lenguaje. En una etapa posterior, se realiza un ajuste en el que se incorpora \n",
    "\n",
    "Luego, en la etapa de ajuste fino, donde al modelo se especializa en tareas específicas de conversación como ChatBot en el que se introduce retroalimentación con un framework de aprendizaje por refuerzo, de forma que se puede mejorar su capacidad para producir respuestas coherentes y cohesivas a partir de las interacciones con los usuarios.\n",
    "\n",
    "Por otro lado, se debe mencionar que a éste tipo de modelo generativos, se les ha denominado en tiempos recientes como *Large Language Models*, donde la palabra inglesa *Large* se emplean por dos razones principales:\n",
    "\n",
    "1) Generalmente necesitan un cantidad de datos de entrenamiento con un volumen enorme, como datos de foros de internet o post de redes sociales y\n",
    "2) Cuentan con una cantidad enorme de parámetros -cantidades numéricas que codifican la información aprende- a ser calibrados. De hecho los modelos actuales rondan el orden de billones de parámetros y por ello  su entrenamiento de modelos requiere de amplio poder computacional.\n",
    "\n",
    "Adicionalmente, podemos añadir que en la práctica obtener un buen resultado a partir de instrucciones de entrada para los LLM's depende además de un diseño adecuado del texto de entrada. Las prácticas y técnicas para optimizar el diseño de las instrucciones que le damos a modelos generativos se denomina **prompt engineering**, algunas recomendaciones de  principios de diseño se pueden consultar en [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api#h_1951f30f08)\n",
    " y [DeepLearning.ai](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ¿Cómo se interactúa con un  LLM's?\n",
    "\n",
    "Por otro lado, cabe destacar que en general, muchas de las compañías que lanzaron sus LLM's al mercado generalmente ponen a disposición de los usuarios una interfaz en un navegador a manera de interacción en conversación de un ChatBot. Además, para desarrollos más complejos también facilitan una manera programática de interacción con un API (*Application Programming Interface*, que no es más que un montón de sistemas de software que hace que dos aplicaciones intercambien información).\n",
    "\n",
    "### 2.3.1 **Interfaces web**\n",
    "\n",
    "Para ejemplificar sus capacidades, a continuación se presenta la interacción con ChatGPT a través de la interfaz de su página web:\n",
    "\n",
    "**Figura 1: Ejemplo de conversación de ChatGPT**\n",
    "\n",
    "![attachment:chatgpt_1.png](./images/chatgpt_1.png)\n",
    "\n",
    "\n",
    "Aquí se obtuvo una respuesta informativa acerca de la pregunta que le hicimos al bot con respecto a los eclipses.\n",
    "\n",
    "Al respecto cabe destacar las siguientes partes de la interacción\n",
    "\n",
    "* **Prompt:** el conjunto de instrucciones de entrada. En esta caso el prompt consistió en el texto `Explica un eclipse solar en términos simples.`\n",
    "* **Respuesta o salida (output):** Se refiere al texto que el Chatbot nos proporcionó como resultado de nuestra instrucción. \n",
    "\n",
    "En general los LLM's tienes algunas limitaciones de uso en términos comerciales y de privacidad, debe revisarse cada caso específico para el detalles\n",
    "\n",
    "### 2.3.2 **Interacción programática por un API**\n",
    "\n",
    "En secciones posteriores abordaremos a detalle como se realiza la interacción por éste medio, pero antes es relevante destacar en general para usar los LLM's a través de API's\n",
    "se debe recurrir a librerias de Python que 1) usan llaves de autenticación que nos permiten comunicar que somos un usuario con licencias o permisos para usar los modelos de los proveedores, 2) contienen la instrucciones de interacción con el ChatBot y 3) reciben la respuestas correspodiente.\n",
    "\n",
    "Entre éstas destacan los siguientes:\n",
    "\n",
    "* **Open AI library:** se trata de la libreria oficial de Open AI para interactura con ChatGPT, véase las instrucciones en https://platform.openai.com/docs/quickstart\n",
    "* **Gemini API SDK:** relativa a la librería de Google para interactuar con Gemini, ver https://ai.google.dev/gemini-api/docs/quickstart?hl=es-419&lang=python.\n",
    "* **Anthropic API SDK:** se refiere a la libreria de Anthropic para usar su modelo, véase https://docs.anthropic.com/en/docs/quickstart-guide.\n",
    "\n",
    "Como se puede notar cada proveedor de LLM y sus librerías específicas para interactuar con sus API's tiene características particulares a las que tendremos que adaptarnos en el desarrollo de nuestros proyectos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.3.2.1 ¿Es necesario usar una librería específica para cada modelo que usemos?\n",
    "\n",
    "Por supuesto que algo que nos puede venir a la cabeza es la disyuntiva siguiente:\n",
    "\n",
    "*¿Cada que use un modelo LLM de un proveedor tengo que cambiar mi código para usar su API?*\n",
    "\n",
    "La respuesta es si y dependemos de que exista una librería que permita interactuar con el API del modelo, junto con sus métodos particulares y diferentes implementaciones, que varían de un proyecto a otro; en su defecto tendremos que crearla de cero.\n",
    "\n",
    "Algunos desarrolladores han tratado de abordar dicho problema desarrollando un framework de abstracción como [LangChaing](https://python.langchain.com/v0.2/docs/introduction/) que facilita la interacción con multiples modelos en el desarrollo de aplicaciones, ya que usar diferentes modelos, testearlos y usarlos en producción con métodos similares, facilitando el despliegue de aplicaciones de forma abstracta e independiente a los modelos subyacentes, desplegar ChatBots y muchas cosas interesantes (Te animamos a leer la documentación del proyecto). \n",
    "\n",
    "Concretamente, las instrucciones para ver como interactuar con diferentes modelos usando el framework LangChain se pueden consultar aquí: https://python.langchain.com/v0.2/docs/tutorials/llm_chain/, además todos los LLM's soportados por diferentes proveedores junto con las instrucciones concretas para su uso están disponibles en la siguiente liga: https://python.langchain.com/v0.1/docs/integrations/llms/\n",
    "\n",
    "Esta es una gran solución, pues permite interactuar con los LLM's y probar sus funcionalidades y concentrarse en lo que queremos que el modelo haga por nuestros casos de uso, en vez de navegar en un mar de código :D!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 ¿Cuáles son los elementos principales de los LLM's?\n",
    "\n",
    "Entrando en materia, debemos reconocer que existen otros elementos que debemos tener presentes para el funcionamiento de tales herramientas y que desempeñan un papel crucial en la generación de texto coherente y contextualmente relevante durante las interacciones con los usuarios.\n",
    "\n",
    "1. **Tokens:** Los tokens son unidades individuales en las que se divide el texto de entrada y salida para que el modelo pueda procesarlo. En el contexto de los LLM's, un token puede ser una palabra, un número, un signo de puntuación o incluso una parte de una palabra. Cada token tiene su propia representación numérica (ver encajes) y, al considerar contextos más amplios, el modelo puede generar respuestas más coherentes y precisas.\n",
    "2. **Encajes (Embeddings):** Los embeddings son representaciones numéricas de las palabras o tokens en el texto. En el caso de los LLM's, se utilizan embeddings para codificar el significado y contexto de las palabras en vectores numéricos de alta dimensión. Esto ayuda al modelo a comprender las relaciones semánticas y sintácticas entre las palabras en las oraciones.\n",
    "3. **Número de tokens máximos:** Para controlar la longitud de las respuestas generadas, se establece un límite en el número máximo de tokens que el modelo puede producir. Esto es útil para garantizar que las respuestas sean de una longitud razonable y fáciles de leer.\n",
    "   \n",
    "4. **Temperatura:** La temperatura es un parámetro de control en el proceso de generación de texto. Un valor alto de temperatura (por ejemplo, 1.0) hace que el modelo sea más creativo y generará respuestas más diversas pero potencialmente menos coherentes. Por otro lado, una temperatura baja (por ejemplo, 0.2) producirá respuestas más determinísticas y más repetitivas. Ajustar la temperatura permite equilibrar la creatividad con la coherencia en las respuestas generadas.\n",
    "\n",
    "Al respecto, conviene notar que en la mayoría de los casos los proveedores de LLM's nos permite configurar la cantidad máxima de tokens en la respuesta generada por el modelo, así como la temperatura correspondiente. Por otro lado, los encajes son parte de la representación numérica calibrar en la fase de pre-entrenamiento del modelo y permite obtener respuesta al traducir las indicaciones que hacemos a chatbot hacia un espacio numérico con el que se generan las respuestas correspondientes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interacción con LLM's a través de API's\n",
    "\n",
    "Como hemos dicho, sumado a la interfaz de interacción de usuario a través de un ChatBot, los proveedores de LLM's también proporciona un API que acepta peticiones HTTP para interactuar con sus servicios. En seguida mostraremos ejemplos usando el framework de librerias específicos de un proveedor y también consideraremos a [LangChaing](https://python.langchain.com/v0.2/docs/introduction/) del que ya hemos hablado anteriormente\n",
    "\n",
    "## 3.1 Generalidades de la interacción con API's de los LLM's\n",
    "\n",
    "### 3.1.1 Licencias de uso y princing\n",
    "\n",
    "Un fenómeno interesante sobrelos modelos LLM es que se han integrado a cadenas de producción de muchas industrias como bienes valiosos; [este fenómeno](https://medium.com/@brandonfrazierjd/the-coming-commoditization-of-large-language-models-7b8b440db488) se denomina *commoditization*.\n",
    "\n",
    "Como resultado, en el mercado actual la mayoria de los proveedores de LLM's comercializan su acceso, ofreciendo diferentes licencias de uso que, en lineas generales, tienen cargos al llamarse cada modelo con una instrucción; por ejemplo, una forma común de su estructura de precios es considerar tarifas que se cobran conforme a la cantidad de tokens de su input y también por la cantidad de tokens que genera la respuesta de la herramienta, esto es, si la entrada es larga pagaremos más y si la salida es larga pagaremos más que en inputs o respuestas cortas.\n",
    "Este punto es **muy importante** porque puede disparar las facturas de uso de los LLM.\n",
    "\n",
    "Sin embargo, algunos proveedores nos permiten usar sus modelos con políticas de prueba o uso justo. Debemos mencionar que a mediados de Junio de 2024, se tiene que algunos proveedores ofrecen interactuar con sus modelos sin costo:\n",
    "\n",
    "1) Claude de Anthropic ofrece una cortesia de 5 USD gratuitos de sus créditos, que expiran en 14 días a partir de la creación del API Key correspondiente. Véase https://console.anthropic.com/settings/keys\n",
    "2) Gemini de Google permite testear sus modelos mediante un API siguiente una política de uso justo. Para ello basta acceder a nuestra cuenta google y proceder liga https://aistudio.google.com/app/apikey\n",
    "\n",
    "\n",
    "Por otro lado, también existe posibilidad de correr modelos LLM en nuestra computadora local, si nuestro hardware cumple con las capacidades necesarios. A continuación se provee una referencia de como usar LangChaing para correr modelos LLM de forma local:\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/guides/development/local_llms/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1.2 ¿Qué modelo LLM debo usar para este Reto?\n",
    "\n",
    "En general, como hemos dichos antes que existen proveedores que ofrecen accesos a sus modelos LLM a través de versiones de prueba (e.g. créditos gratuitos) o a través de licencias con uso restringido.\n",
    "\n",
    "Es por ello que para este reto se recomienda amplimente explorar las páginas de los principales proveedores y localizar aquellos que con versiones de prueba y sin costo bajo ciertas condiciones, generando la correspondiente API Key. También se puede considerar opciones como las que siguen:\n",
    "\n",
    "1. Gemini de Google y Claude Anthropic, que a mediados de 2024 ofrecen versiones de prueba,\n",
    "2. Cuentas de tipo educativo o capaz gratuitas (Free Tier) de proveedores en la nube que permiten hacer uso de modelos LLM's, por ejemplo:\n",
    "    - AWS Educate con las cuales se puede hacer uso del servicio [Amazon Bedrock](https://aws.amazon.com/es/bedrock/) que nos da acceso a muchos modelos de inteligencia articifial generativa,\n",
    "    - Vertex AI de Google, usando créditos de Google Cloud Computing\n",
    "    - Azure ML de Microsoft, usando créditos de Azure.\n",
    "3. Versiones de LLM's que puede correr localmente (ver sección 2.3.2.1), sin embargo es sabido que requieren de computadoras con capacidades grandes de hardware como tarjetas GPU.\n",
    "4. En caso de tener acceso claves de un LLM para uso propio, usarlas cuidando el uso limitado de tokens para no incrementar demasiado el costo de uso.\n",
    "\n",
    "Acerquese al Experto Técnico para orientación de la mejor opción en su caso concreto :D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1.3 Autenticación\n",
    "\n",
    "En general, los proveedores nos dejan interactuar con las API's de los LLM a través de la creación de un API Key para identificar al usuario que hará las peticiones correspondientes, la cual no es mas que una clave alfanumerica que le comunicaremos a la libreria y con la cual podremos usar servicios.\n",
    "\n",
    "\n",
    "Para crearla, basta ingresar en la dirección electrónica del proveedor y buscar el apartado de creación correspondiente. \n",
    "\n",
    "A manera de ejemplo, en la imagen inferior se aprecia como se ha creado un API para Gemini, la cual se puede crear en la liga https://aistudio.google.com/app/apikey:\n",
    "\n",
    "![title](./images/gemini_api_key.png)\n",
    "\n",
    "\n",
    "A partir de este punto, es importante que salvemos copiemos y salvemos la llave en una ubicación segura, pues será la única ocasión en que ésta información será visible para nosotros dentro de la plataforma.\n",
    "\n",
    "Cabe destacar que muchos proveedores LLM's, como OpenAI, proveen consejos acerca de las mejores prácticas para el manejo seguro de las API Key de la aplicación (véase https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety), pero que, como es usual en el desarrollo de software, se resumen en 1) controlar el acceso a las claves, 2) no colocarlas en ambientes donde se puedan compartir de forma no intenciona y 3) leerlas en nuestro código desde variables de ambiente del sistema operativo para que no se encuentren expuestas en repositorios de trabajo. **Nota:** Es extremadamente importante seguir las recomendaciones de seguridad para evitar sorpresas!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Modelos\n",
    "\n",
    "Como es usual, los proveedores de LLM's van perfeccionando sus modelos, algunos para mejorar su eficiencia, optimizar la cantidad de parámetros o especializarse en temas concretos. Estos suelen estar disponibles para los usuarios a través de versiones y normalmente su tarifa puede variar siendo más caras las versiones más actuales.\n",
    "\n",
    "Por ejemplo, OpenAI provee una lista de sus modelos https://platform.openai.com/docs/models/, entre los que a mediados de 2024 destacan:\n",
    "\n",
    "* **GPT-4o:** https://platform.openai.com/docs/models/gpt-4o\n",
    "* **GPT-4 Turbo and GPT-4:** https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\n",
    "* **GPT-3.5 Turbo:** https://platform.openai.com/docs/models/gpt-3-5-turbo\n",
    "\n",
    "En la interacción con el API deberemos establecer con que modelo queremos interactuar a través de su nombre.\n",
    "\n",
    "En tal sentido, es muy importante remarcar que el acceso las versiones de diferentes modelo a través del API tendrá en general un modelo de **cobro basado en la longitud de tokens en los inputs y outputs de la interacción**. En general, se trata de precio en milésimas o centavos de dolar en paquetes de miles o millones tokens en el input y output. No se debe perder de vista el uso que se hace de la herramienta para evitar cobros inesperados. Para mayor información consultar la sección correspondiente de pricing del modelo que se vaya a emplear.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Haciendo peticiones hacia el API con librerias\n",
    "\n",
    "Para esta sección, interactuaremos con el LLM llamado Gemini de Google, usando 1) la libreria nativa desarrollada por dicho proveedor, y 2) el framework de LangChain.\n",
    "\n",
    "\n",
    "### 3.1.5.1 Usando la librería de un proveedor\n",
    "\n",
    "En este caso, se recomienda  localizar la documentación del proveedor de LLM que se pretende usar junto con las intrucciones correspondiente. \n",
    "\n",
    "En nuestro ejemplo, Gemini provee la documentación en la liga https://ai.google.dev/gemini-api/docs/get-started/tutorial?hl=es-419&lang=python\n",
    "\n",
    "En resumen, esta indica que podemos instalar el SDK correspondiente bajo la instrucción:\n",
    "\n",
    "```\n",
    "# Instala libreria de google-generativeai para Gemini\n",
    "pip install -q -U google-generativeai\n",
    "```\n",
    "\n",
    "A continuación mostramos un código simple para interactuar con la API donde introducimos un prompt para conocer sobre *Mario Bross*. La idea general del interacción con la API es simple: i) importamos el módulo para interacturar con el modelo, ii) le damos el API Key, definimos el nombre del modelo a usar, iii) le damos el prompt correspondiente y iv) recibimos la respuesta en JSON que deberemos post-procesar para extraer el texto y demás datos relevantes de la interacción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Mario Bros es un personaje de videojuegos creado por Shigeru Miyamoto, protagonista de la franquicia Super Mario. Es un fontanero italiano ficticio que vive en el Reino Champi\\u00f1\\u00f3n, donde debe rescatar a la Princesa Peach del malvado Bowser. Es uno de los personajes de videojuegos m\\u00e1s populares y reconocibles del mundo. \\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 10,\n",
      "        \"candidates_token_count\": 67,\n",
      "        \"total_token_count\": 77\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script para interactuar con el API de Gemini en su version\n",
    "gemini-1.5-flash, LLM desarrollado por Google Deep Mind\n",
    "\"\"\"\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Indica el API Key de nuestro usuario\n",
    "GOOGLE_API_KEY=\"...\" # <-- aqui va tu clave\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Especificamos el modelo a emplear ('gemini-1.5-flash')\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash'\n",
    "    )\n",
    "# Prompt a enviar al modelo\n",
    "PROMT=\"Dime en tres lineas quien es Mario Bross\"\n",
    "\n",
    "# Obtiene respuesta\n",
    "response = model.generate_content(PROMT)\n",
    "\n",
    "print(\"Responde en valor crudo\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el API retorna una respuesta compleja en formato JSON, sin embargo la respuesta del texto se puede extraer y es algo parecido a lo siguiente:\n",
    "\n",
    "\n",
    "*Mario Bros es un personaje de videojuegos creado por Shigeru Miyamoto, protagonista de la franquicia Super Mario. Es un fontanero italiano ficticio que vive en el Reino Champiñon, donde debe rescatar a la Princesa Peach del malvado Bowser. Es uno de los personajes de videojuegos más populares y reconocibles del mundo.*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas ideas se pueden extrapolar a cualquier librería o SDK del proveedor de LLM que nos interese usar, dado que la sintaxis suele ser similar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5.2 Usando el framework de LangChain\n",
    "\n",
    "En complemento, también podemos optar por usar el framework de LangChain, que como sabemos nos ayuda a abstraer la interacción con los LLM's.\n",
    "\n",
    "Reiteramos que 1) las instrucciones para ver como interactuar con diferentes modelos usando el framework LangChain se pueden consultar aquí: https://python.langchain.com/v0.2/docs/tutorials/llm_chain/, y 2) además todos los LLM's soportados por diferentes proveedores junto con las instrucciones concretas para su uso están disponibles en la siguiente liga: https://python.langchain.com/v0.1/docs/integrations/llms/.\n",
    "\n",
    "Adicionalmente, existen gran cantidad de recursos formativos en la documentación respecto a las capacidades de LangChain:\n",
    "\n",
    "* **How-to guides:** https://python.langchain.com/v0.2/docs/how_to/\n",
    "* **Quick start on LLM's:** https://python.langchain.com/v0.1/docs/modules/model_io/llms/quick_start/\n",
    "* **Components:** https://python.langchain.com/v0.1/docs/modules/#prompts\n",
    "* **Chat models (including Messagges):** https://python.langchain.com/v0.1/docs/modules/model_io/chat/quick_start/\n",
    "* **Parsers:** https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/\n",
    "\n",
    "\n",
    "Siguiendo tales documentaciones se espera que pueda adaptar el código correspondiente para el proveedor de LLM con el que desee trabajar.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro siguiente ejemplo, interactuaremos nuevamente con Gemini de Google. Primero, de acuerdo a la documentación deberemos ejecutar los siguientes códigos para instalar las librerías correspondientes:\n",
    "\n",
    "\n",
    "```\n",
    "# Instalamos langchain\n",
    "pip install langchain\n",
    "\n",
    "# Instala el soporte para el modelo que nos interesa\n",
    "pip install --upgrade --quiet  langchain-google-genai pillow\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora mostraremos un ejemplo más complejo que lo anterior aprovechando las funcionalidades que nos da LangChaing. En concreto éste código realizará lo siguiente\n",
    "\n",
    " 1) Usaremos la funcionlidad *langchain_core.messages* que permite interactuar con diferentes tipos de objetos de prompt y conversaciones (ver https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.messages) para enviar mensajes de sistema e instrucciones de un humano,\n",
    "\n",
    "2) Haremos un *parsing* de la respuesta (es decir, la post-procesaremos para poner los datos que recibimos del API en un formato más cómodo). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquí hay 3 lemas para un equipo de béisbol con un zorro como mascota:\n",
      "\n",
      "1. **Desata al zorro: ¡Ataca!**\n",
      "2. **Zorros astutos: ¡Dominan el diamante!**\n",
      "3. **¡El zorro te atrapa: ¡Cuidado!** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Clase para hacer parsing de la respuesta\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Definimos tipos de mensajes a enviar al modelo\n",
    "messages = [\n",
    "    SystemMessage(content=\"Ayudame a crear 3 propuestas de slogan para la siguiente situacion\"),\n",
    "    HumanMessage(content=\"un equipo de baseball que tiene a un zorro como mascota en 5 palabras\"),\n",
    "]\n",
    "\n",
    "# Indica el API Key de nuestro usuario\n",
    "GOOGLE_API_KEY = \"...\" # <-- aqui va tu clave\n",
    "\n",
    "# Define el modelo a emplear\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model='gemini-1.5-flash',\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Ejecuta una tarea de completar un texto\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# Imprime el mensaje respuesta del LLM\n",
    "print(parser.invoke(result))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:**\n",
    "\n",
    "    * Recordemos que los LLM se basan en un modelo generativo de lenguaje que tiene una componente que controla la aleatoreidad del texto generado (`temperature`), por lo que no siempre obtendremos las mismas respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquí tienes 3 lemas para un equipo de béisbol con un zorro como mascota, en 5 palabras o menos:\n",
      "\n",
      "1. **Desata la astucia del zorro.**\n",
      "2. **El zorro te atrapa.**\n",
      "3. **Rápido, astuto, zorro.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indica el API Key de nuestro usuario\n",
    "GOOGLE_API_KEY = \"...\" # <-- aqui va tu clave\n",
    "\n",
    "# Define el modelo a emplear cambiado la temperatura\n",
    "\n",
    "model2 = ChatGoogleGenerativeAI(\n",
    "    model='gemini-1.5-flash',\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.3 # <-- aqui se cambio la temperatura\n",
    ")\n",
    "\n",
    "# Ejecuta una tarea de completar un texto\n",
    "result2 = model2.invoke(messages)\n",
    "\n",
    "# Imprime el mensaje respuesta del LLM\n",
    "print(parser.invoke(result2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extracción de texto en diferentes formatos\n",
    "\n",
    "Python es un lenguaje con mucha flexibilidad para trabajar archivos de texto. A continuación se presenta una discusión de formatos comunmente encontrados en la práctica **.txt**, **.pdf** y **.html**.\n",
    "\n",
    "## 4.1 Archivos .txt\n",
    "\n",
    "Los archivos de texto (.txt) son los más fáciles de manejar pues no necesitan ninguna biblioteca especial. Se pueden acceder desde la función `open(...)`, además de que es posible realizar manipulaciones como reemplazar caracteres, añadir nuevos, realizar lecturas y escrituras del mismo.\n",
    "\n",
    "\n",
    "Una buena referencia para trabajar con éste tipo de archivo se encuentra en https://realpython.com/read-write-files-python/\n",
    "\n",
    "A continuación se presenta un ejemplo de cómo leer el archivo `tale_of_two_cities_chapter_1.txt` que posee el texto del primer capítulo de la novela *Tales of Two Cities* del autor Charles Dickens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I.\n",
      "The Period\n",
      "\n",
      "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n",
      "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever.\n",
      "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood.\n",
      "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it. Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers’ warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of “the Captain,” gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, “in consequence of the failure of his ammunition:” after which the mail was robbed in peace; that magnificent potentate, the Lord Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despoiled the illustrious creature in sight of all his retinue; prisoners in London gaols fought battles with their turnkeys, and the majesty of the law fired blunderbusses in among them, loaded with rounds of shot and ball; thieves snipped off diamond crosses from the necks of noble lords at Court drawing-rooms; musketeers went into St. Giles’s, to search for contraband goods, and the mob fired on the musketeers, and the musketeers fired on the mob, and nobody thought any of these occurrences much out of the common way. In the midst of them, the hangman, ever busy and ever worse than useless, was in constant requisition; now, stringing up long rows of miscellaneous criminals; now, hanging a housebreaker on Saturday who had been taken on Tuesday; now, burning people in the hand at Newgate by the dozen, and now burning pamphlets at the door of Westminster Hall; to-day, taking the life of an atrocious murderer, and to-morrow of a wretched pilferer who had robbed a farmer’s boy of sixpence.\n",
      "All these things, and a thousand like them, came to pass in and close upon the dear old year one thousand seven hundred and seventy-five. Environed by them, while the Woodman and the Farmer worked unheeded, those two of the large jaws, and those other two of the plain and the fair faces, trod with stir enough, and carried their divine rights with a high hand. Thus did the year one thousand seven hundred and seventy-five conduct their Greatnesses, and myriads of small creatures—the creatures of this chronicle among the rest—along the roads that lay before them.\n"
     ]
    }
   ],
   "source": [
    "def reading_txt(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el texto de un archivo .txt\n",
    "\n",
    "    Parametros:\n",
    "        file_path (str): Ruta del archivo de texto a analizar.\n",
    "\n",
    "    Salida:\n",
    "        str: El texto extraído del archivo.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Ruta donde se encuentra el archivo\n",
    "txt_file_path = './data/tale_of_two_cities_chapter_1.txt'\n",
    "\n",
    "# Extraemos el texto\n",
    "extracted_text = reading_txt(txt_file_path)\n",
    "\n",
    "# Imprime el texto\n",
    "print(extracted_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Archivos .pdf\n",
    "\n",
    "El formato de documento portable (*.pdf*, por sus siglas en inglés) es tipo de archivo con gran popularidad en el mundo. En Python, se\n",
    "suele emplear la librería `PyPDF2` (ver https://pypdf2.readthedocs.io/en/3.0.0/).\n",
    "\n",
    "```\n",
    "pip install PyPDF2\n",
    "```\n",
    "\n",
    "Una referencia de como emplearlo para extraer texto se presenta en https://pypdf2.readthedocs.io/en/3.0.0/user/extract-text.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Archivos .html\n",
    "\n",
    "El formato *Hypertext Markup Language* (*.html*, por sus siglas en inglés) permite crear documento con formato estilizado en el ambiente de desarrollo web, por lo que multiples sitios electrónicos basan si diseño en el mismo.  En Python, se puede extraer texto de dicho formato empleando la libreria denominada `Beautiful Soup` (ver https://www.crummy.com/software/BeautifulSoup/bs4/doc/), la cual se puede instalar como sigue:\n",
    "\n",
    "```\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "La tarea de extracción en este caso depende fuertemente de la estructura en que el texto se encuentra organizado en el archivo .html (por ejemplo en forma de párrafos, viñetas, tablas y otros), las técnicas para extraerlo de manera eficiente se denominan *web scrapping*. Una referencia de como emplearlo para extraer texto se presenta en https://realpython.com/beautiful-soup-web-scraper-python/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entregables\n",
    "\n",
    "En esta sección se describen los entregables de la presente etapa que consisten en un script en Python para interactuar con algún LLM a través de su correspondiente API. \n",
    "\n",
    "Para ello se deberá crear las cuentas de plataforma correspondiente y generar el API Key correspondiente (**NO SE DEBE INCLUIR LA API KEY**, revise, por ejemplo, https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety para ver como invocarle como variable de ambiente desde Python).\n",
    "\n",
    "**Nota:** *En caso de tener dudas del modelo que puede ser más adecuado para usar y del proceso para interactuar con el usado el API se debe consultar al Experto Técnico*\n",
    "\n",
    "---\n",
    "\n",
    "Considere las instrucciones siguientes para los entregables:\n",
    "\n",
    "A. Diseña scripts y prompts que permitan:\n",
    "\n",
    "    1) Obtener un resumen en español, de dos párrafos de longitud, del contenido del texto `news_digital_bank.txt`. Adicionalmente, se deberá incluir un tercer párrafo que indique cuál es el diario del que proviene el texto y el título correspondiente de la noticia. Este programa se deberá guardar con nombre `conversacion_1.py`\n",
    "\n",
    "    2) Crear 5 viñetas (bullets) que presenten los elementos más importantes de la historia el archivo `cuento.pdf` (usando el texto de todas las páginas del archivo). Adicionalmente, se deberá incluir un par de viñetas que indique a) el nombre del autor del texto, 2) los personajes principales de la trama, 3) el título del cuento. Dicho programa se deberá guardar con nombre `conversacion_2.py`\n",
    "\n",
    "Cabe destacar que como resultado de los programas anteriores, se debe crear un script de conversación que guarde el script de conversación entre el usuario y LLM en un archivo .txt (conversacion_i.txt donde i es el número de inciso asociado), con la estructura siguiente:\n",
    "\n",
    "**Ejemplo**\n",
    "```\n",
    "Usuario: <Indicaciones del prompt empleado>\n",
    "\n",
    "LLM: Respuesta\n",
    "```\n",
    "\n",
    "Adicionalmente se deberá adjuntar capturar de pantalla en formato .png donde se aprecia el cuerpo de las conversaciones generadas por los ChatBots, se pueden usar numeraciones sucesivas sin son muchas fotos, ejemplo: evidencia_1_conversacion_i.png, evidencia_2_conversacion_i.png, ..., evidencia_5_conversacion_i.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
